## Results

### Structural variation in vg

We used vg to implement a straightforward SV genotyping pipeline.
Reads are mapped to the graph and used to compute the read support for each node and edge (see [Supplementary Information](#variation-graph-and-structural-variation) for a description of the graph formalism).
Sites of variation within the graph are then identified using the snarl decomposition as described in [@tag:snarls].
These sites correspond to intervals along the reference paths (ex. contigs or chromosomes) which are embedded in the graph.
They also contain nodes and edges deviating from the reference path, which represent variation at the site.
For each site, the two most supported paths spanning its interval (haplotypes) are determined, and their relative supports used to produce a genotype at that site (Fig. {@fig:1}a).
The pipeline is described in detail in [Methods](#simulation-experiment).
We rigorously evaluated the accuracy of our method on a variety of datasets, and present these results in the remainder of this section.

### Simulated dataset

As a proof of concept, we simulated genomes and different types of SVs with a size distribution matching real SVs[@tag:hgsvc].
We compared vg against Paragraph, SVTyper, Delly Genotyper, and BayesTyper across different levels of sequencing depth.
We also added some errors (1-10bp) to the location of the breakpoints to investigate their effect on genotyping accuracy (see [Methods](#simulation-experiment)).
The results are shown in Fig. {@fig:1}b.

When using the correct breakpoints, most methods performed similarly, with differences only becoming visible at very low sequencing depths.
Only vg and Paragraph maintained their performance in the presence of 1-10 bp errors in the breakpoint locations. 
The dramatic drop for BayesTyper can be explained by its k-mer-based approach that requires precise breakpoints.
Overall, these results show that vg is capable of genotyping SVs and is robust to breakpoint inaccuracies in the input VCF.

### HGSVC dataset

72,485 structural variants from The Human Genome Structural Variation Consortium (HGSVC) were used to benchmark the genotyping performance of vg against the four other SV genotyping methods.
This high-quality SV catalog was generated from three samples using a consensus from different sequencing, phasing, and variant calling technologies[@tag:hgsvc]. 
The three individual samples represent different human populations: Han Chinese (HG00514), Puerto-Rican (HG00733), and Yoruban Nigerian (NA19240).
We used these SVs to construct a graph with vg and as input for the other genotypers.
Using short sequencing reads, the SVs were genotyped and compared with the genotypes in the original catalog (see [Methods](#hgsvc-analysis)).

First we compared the methods using simulated reads for HG00514.
This represents the ideal situation where the SV catalog exactly matches the SVs supported by the reads.
BayesTyper and vg showed the best F1 score and precision-recall trade-offs (Fig. {@fig:2}a and Additional File 1: Fig. {@fig:hgsvc-sim-geno}, Additional File 1: Table {@tbl:hgsvc}), outperforming the other methods by a clear margin.
When restricting the comparisons to regions not identified as tandem repeats or segmental duplications, the genotyping predictions were significantly better for all methods.
We observed similar results when evaluating the presence of an SV call instead of the exact genotype (Fig. {@fig:2}a and Additional File 1: Fig. {@fig:hgsvc-sim}).

We then repeated the analysis using real Illumina reads from the three HGSVC samples to benchmark the methods on a more realistic experiment.
Here, vg clearly outperformed other approaches (Fig. {@fig:2}a and Additional File 1: Fig. {@fig:hgsvc-real-geno}).
In non-repeat regions and insertions across the whole genome, the F1 scores and precision-recall AUC were higher for vg compared to other methods.
For example, for deletions in non-repeat regions, the F1 score for vg was 0.824 while the second best method, Paragraph, had a F1 score of 0.717.
We observed similar results when evaluating the presence of an SV call instead of the exact genotype (Fig. {@fig:2}a and Additional File 1: Fig. {@fig:hgsvc-real}).

In general, the genotyped variants were matched 1-to-1 with variants in the truth set but some methods showed some signs of "over-genotyping" that is not reflected in the precision/recall/F1 scores.
Methods like Paragraph, Delly Genotyper or SVTyper tended to genotype on average more than one variant per truth-set variant (Additional File 1: Fig. {@fig:eval-matchtp}).
Like other SV catalogs, the HGSVC catalog is not fully sequence-resolved and contains a number of near-duplicates with slightly different breakpoint definition.
When genotyping a sample, multiple versions of a variant are genotyped multiple times by methods that analyze each variant independently.
In contrast, vg follows a unified path-centric approach that only select the best genotype in a region (see [Methods](#sv-genotyping-algorithm)). 

We further evaluate the performance for different SV sizes and repeat content.
In addition, vg's performance was stable across the spectrum of SV sizes (Figs. {@fig:2}b-c).
By annotating the repeat content of the deleted/inserted sequence we further evaluated vg's performance across repeat classes. 
As expected, simple repeat variation was more challenging to genotype than transposable element polymorphisms (Additional File 1: Fig. {@fig:eval-rmsk}). 
Fig. {@fig:hgsvc-ex} shows an example of an exonic deletion that was correctly genotyped by vg but not by BayesTyper, SVTyper or Delly Genotyper.

### Other long-read datasets

#### Genome in a Bottle Consortium

The Genome in a Bottle (GiaB) consortium is currently producing a high-quality SV catalog for an Ashkenazim individual (HG002)[@doi:10.1038/sdata.2016.25;@doi:10.1038/s41587-019-0074-6;@doi:10.1101/664623].
Dozens of SV callers operating on datasets from short, long, and linked reads were used to produce this set of SVs.
We evaluated the SV genotyping methods on this sample as well using the GIAB VCF, which also contains parental calls (HG003 and HG004), all totaling 30,224 SVs.
Relative to the HGSVC dataset, vg performed similarly but Paragraph saw a large boost in accuracy and was the most accurate method across all metrics.  (Fig. {@fig:2}, Additional File 1: Figs. {@fig:giab-geno} and {@fig:giab}, and Additional File 1: Table {@tbl:giab}).
As before, the remaining methods produced lower F1 scores.

#### SMRT-SV v2 catalog and training data [@tag:audano2019]

A recent study by Audano et al. generated a catalog of 97,368 SVs (referred as SVPOP below) using long-read sequencing across 15 individuals[@tag:audano2019].
These variants were then genotyped from short reads across 440 individuals using the SMRT-SV v2 Genotyper, a machine learning-based tool implemented for that study.
The SMRT-SV v2 Genotyper was trained on a pseudo-diploid genome constructed from high quality assemblies of two haploid cell lines (CHM1 and CHM13) and a single negative control (NA19240).
We first used vg to genotype the SVs in this two-sample training dataset using 30X coverage reads, and compared the results with the SMRT-SV v2 Genotyper.
vg was systematically better at predicting the presence of an SV for both SV types, but SMRT-SV v2 Genotyper produced slightly better genotypes for deletions in the whole genome(see Fig. {@fig:chmpd-svpop}, Additional File 1: Figs. {@fig:chmpd-geno} and {@fig:chmpd}, and Additional File 1: Table {@tbl:chmpd}). 
To compare vg and SMRT-SV v2 Genotyper on a larger dataset, we then genotyped SVs from the entire SVPOP catalog with both methods, using the read data from the three HGSVC samples described above.
Given that the SVPOP catalog contains these three samples, we once again evaluated accuracy by using the long-read calls as a baseline.
Paragraph was included as an additional point of comparison. 

Compared to SMRT-SV v2 Genotyper, vg had a better precision-recall curve and a higher F1 for both insertions and deletions (SVPOP in Fig. {@fig:chmpd-svpop} and Additional File 1: Fig. {@fig:svpop}, and Additional File 1: Table {@tbl:svpop}).
Paragraph's performance was virtually identical to vg's.
Of note, SMRT-SV v2 Genotyper produces *no-calls* in regions where the read coverage is too low, and we observed that its recall increased when filtering these regions out the input set.
Interestingly, vg performed well even in regions where SMRT-SV v2 Genotyper produced *no-calls* (Additional File 1: Fig. {@fig:svpop-regions} and Additional File 1: Table {@tbl:svpop-regions}).
Audano et al. discovered 217 sequence-resolved inversions using long reads, which we attempted to genotype.
vg correctly predicted the presence of around 14% of the inversions present in the three samples (Additional File 1: Table {@tbl:svpop}).
Inversions are often complex, harboring additional variation that makes their characterization and genotyping challenging.

### Graphs from alignment of *de novo* assemblies

We can construct variation graphs directly from whole genome alignments (WGA) of multiple *de novo* assemblies[@tag:vgnbt].
This bypasses the need for generating an explicit variant catalog relative to a linear reference, which could be a source of error due to the reference bias inherent in read mapping and variant calling.
Genome alignments from graph-based software such as Cactus [@doi:10.1101/gr.123356.111] can contain complex structural variation that is extremely difficult to represent, let alone call, outside of a graph, but which is nevertheless representative of the actual genomic variation between the aligned assemblies.
We sought to establish if graphs built in this fashion provide advantages for SV genotyping.

To do so, we analyzed public sequencing datasets for 12 yeast strains from two related clades (*S. cerevisiae* and *S. paradoxus*) [@doi:10.1038/ng.3847].
We distinguished two different strain sets, in order to assess how the completeness of the graph affects the results. 
For the *all strains set*, all 12 strains were used, with *S.c. S288C* as the reference strain.
For the *five strains set*, *S.c. S288C* was used as the reference strain, and we selected two other strains from each of the two clades (see [Methods](#yeast-graph-analysis)).
We compared genotyping results from a WGA-derived graph (*cactus graph*) with results from a VCF-derived graph (*VCF graph*).
The *VCF graph* was created from the linear reference genome of the *S.c. S288C* strain and a set of SVs relative to this reference strain in VCF format identified from the other assemblies in the respective strain set by three methods: Assemblytics [@doi:10.1093/bioinformatics/btw369], AsmVar [@doi:10.1186/s13742-015-0103-4] and paftools [@doi:10.1093/bioinformatics/bty191].
The *cactus graph* was derived from a multiple genome alignment of the strains in the respective strain set using Cactus [@doi:10.1101/gr.123356.111].
The *VCF graph* is mostly linear and highly dependent on the reference genome.
In contrast, the *cactus graph* is structurally complex and relatively free of reference bias.

First, we tested our hypothesis that the *cactus graph* has higher mappability due to its better representation of sequence diversity among the yeast strains (see [Supplementary Information](#mappability-comparison-between-yeast-graphs)).
Generally, more reads mapped to the *cactus graph* with high identity (Additional File 1: Figs. {@fig:panel3}a and {@fig:panel5}a) and high mapping quality (Additional File 1: Figs. {@fig:panel3}b and {@fig:panel5}b) than to the *VCF graph*.
On average, 88%, 79%, and 68% of reads mapped to the *all strain cactus graph* with an identity of at least 50%, 90%, and 100%, respectively, compared to only 77%, 57%, and 23% of reads on the *all strain VCF graph*.
Similarly, 88% of reads mapped to the *all strain cactus graph* with a mapping quality of at least 30 compared to only 80% of reads on the *all strain VCF graph*.

Next, we compared the SV genotyping performance of both graph types.
We mapped short reads from the 11 non-reference strains to both graphs and genotyped variants for each strain using the vg toolkit's variant calling module (see [Methods](#sv-genotyping-algorithm)).
There is no gold standard available for these samples to compare against which renders an evaluation using recall, precision and F1 score impossible.
Therefore, we used an indirect measure of SV genotyping accuracy.
We evaluated each SV genotype set based on the alignment of reads to a *sample graph* constructed from the genotype set (see [Methods](#genotyping-of-svs)).
Conceptually, the sample graph represents the sample's diploid genome by starting out from the reference genome and augmenting it with the genotype results.
If a given genotype set is correct, we expect that reads from the same sample will be mapped with high identity and confidence to the corresponding sample graph.
To specifically quantify mappability in SV regions we excluded reads that produced identical mapping quality and identity on both sample graphs and an empty sample graph containing the linear reference only (see [Methods](#genotyping-of-svs) and Additional File 1: Fig. {@fig:panel6} for results from all reads).
Then, we analyzed the average delta in mapping identity and mapping quality of the remaining short reads between both sample graphs (Figs. {@fig:4}a and b).

For most of the strains, we observed an improvement in mapping identity of the short reads on the *cactus sample graph* compared to the *VCF sample graph*.
The mean improvement in mapping identity across the strains (for reads differing in mapping identity) was 8.0% and 8.5% for the *all strains set* graphs and the *five strains set* graphs, respectively.
Generally, the improvement in mapping identity was larger for strains in the *S. paradoxus* clade (mean of 13.7% and 13.3% for the two strain sets, respectively) than for strains in the *S. cerevisiae* clade (mean of 3.3% and 4.4%).
While the higher mapping identity indicated that the *cactus graph* represents the reads better (Fig. {@fig:4}a), the higher mapping quality confirmed that this did not come at the cost of added ambiguity or a more complex graph (Fig. {@fig:4}b).
For most strains, we observed an improvement in mapping quality of the short reads on the *cactus sample graph* compared to the *VCF sample graph* (mean improvement across the strains of 1.0 and 5.7 for the two strain sets, respectively).
